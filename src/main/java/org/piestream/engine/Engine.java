package org.piestream.engine;

import org.apache.kafka.streams.kstream.ForeachAction;
import org.piestream.events.PointEvent;
import org.piestream.merger.HashJoiner;
import org.piestream.merger.MapMerger;
import org.piestream.merger.Table;
import org.piestream.parser.Schema;
import org.piestream.events.Attribute;
import org.piestream.parser.MPIEPairSource;
import org.piestream.parser.QueryParser;
import org.piestream.piepair.eba.EBA;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.util.List;

/**
 * Engine class processes incoming events and applies a series of transformations, merges, 
 * and updates based on parsed query patterns.
 * It manages preprocessing, event processing, and timing metrics for different stages 
 * in the processing pipeline.
 */
public class Engine implements ForeachAction<String, String> {

    // Logger for this class
    private static final Logger logger = LoggerFactory.getLogger(Engine.class);

    // Event preprocessing instance
    private final EventPreprocessor processor;
    // Partition attribute used to partition the incoming data stream
    private final Attribute partitionAttribute;
    // List of pattern sources generated by the query parser
    private final List<MPIEPairSource> MPPSourceList;
    // Worker instance that performs event processing tasks
    private final Worker worker;
    // Query parser to parse the incoming query for pattern matching
    private final QueryParser parser;

    // Cumulative time metrics for different processing stages
    private long preprocessTime = 0;
    private long runOneByOneTime = 0;
    private long deriveRelTime = 0;
    private long mergeTime = 0;
    private long updateTime = 0;

    /**
     * Constructs the Engine with the specified schema, partition attribute, query, and window type.
     *
     * @param schema Schema describing the data structure
     * @param partitionAttribute Attribute used for partitioning the data stream
     * @param query Query string defining the processing logic
     * @param winType Type of window used for event processing
     */
    public Engine(Schema schema, Attribute partitionAttribute, String query, WindowType winType) {
        this.parser = new QueryParser(query, schema);
        try {
            // Parse the query to extract pattern clauses
            parser.parse();
        } catch (QueryParser.ParseException | EBA.ParseException e) {
            logger.error("Failed to parse query: " + e.getMessage());
        }
        // Retrieve pattern sources from parsed query
        this.MPPSourceList = parser.getPatternClause();
        long windowCapacityUnitNS = parser.getwindowClause();
        // Initialize the event preprocessor
        this.processor = new EventPreprocessor(schema);
        this.partitionAttribute = partitionAttribute;
        // Initialize window for event processing
        Window window = new Window(winType, windowCapacityUnitNS, schema.getTimestampUnit().getNanosPerUnit());
        // Initialize the worker to handle event processing
        this.worker = new Worker(MPPSourceList, window, parser.getEBA2String());
    }

    /**
     * Alternate constructor that uses default window type CAPACITY_WINDOW.
     *
     * @param schema Schema describing the data structure
     * @param query Query string defining the processing logic
     */
    public Engine(Schema schema, String query) {
        this(schema, null, query, WindowType.CAPACITY_WINDOW);
    }

    /**
     * Alternate constructor that accepts a specific window type.
     *
     * @param schema Schema describing the data structure
     * @param query Query string defining the processing logic
     * @param winType Type of window used for event processing
     */
    public Engine(Schema schema, String query, WindowType winType) {
        this(schema, null, query, winType);
    }

    /**
     * Applies the event processing logic to a key-value pair.
     * The method includes preprocessing, event handling, relationship derivation, 
     * merging, and data updates. Each step is timed to measure performance.
     *
     * @param key The key associated with the event
     * @param value The event data (in string form)
     */
    public void apply(String key, String value) {
        long startTime, endTime;

        // Preprocessing: Parse the event value into a PointEvent object
        startTime = System.currentTimeMillis();
        PointEvent pointEvent = processor.preprocess(value);
        PointEvent pe = new PointEvent(pointEvent);
        endTime = System.currentTimeMillis();
        preprocessTime += (endTime - startTime);

        // Handle event one by one
        startTime = System.currentTimeMillis();
        worker.resetBeforeRun(pe.getTimestamp());
        worker.runOneByOne(pe);
        endTime = System.currentTimeMillis();
        runOneByOneTime += (endTime - startTime);

        // Derive before-after relationships
        startTime = System.currentTimeMillis();
        worker.deriveBeforeAfterRel();
        endTime = System.currentTimeMillis();
        deriveRelTime += (endTime - startTime);

        // Perform merging of processed data
        startTime = System.currentTimeMillis();
        worker.mergeAfterRun();
        endTime = System.currentTimeMillis();
        mergeTime += (endTime - startTime);

        // Update internal data structures with the new results
        startTime = System.currentTimeMillis();
        worker.updateData();
        endTime = System.currentTimeMillis();
        updateTime += (endTime - startTime);
    }

    /**
     * Prints the accumulated time spent on various stages of the processing pipeline.
     * Includes total time, and breakdown for each major step in the pipeline.
     */
    public void printAccumulatedTimes() {
        long duration = preprocessTime + runOneByOneTime + deriveRelTime + mergeTime + updateTime;
        logger.info("ALL Processing time: " + (double) duration / 1000 + " s");
        logger.info("Total preprocess time: " + (double) preprocessTime / 1000 + " s");
        logger.info("Total run one by one time: " + (double) runOneByOneTime / 1000 + " s (" + Math.round((double) runOneByOneTime / (double) duration * 100) + "%)");
        logger.info("Total derive before-after relationship time: " + (double) deriveRelTime / 1000 + " s (" + Math.round((double) deriveRelTime / (double) duration * 100) + "%)");
        logger.info("Total merge after run time: " + (double) mergeTime / 1000 + " s (" + Math.round((double) mergeTime / (double) duration * 100) + "%)");
        logger.info("    normalJoin Time: " + (double) this.worker.getTree().getMerger().normalJoin / 1000 + " s (" + Math.round((double) this.worker.getTree().getMerger().normalJoin / (double) duration * 100) + "%)");
        logger.info("        searchForJoin Time: " + (double) HashJoiner.searchForJoin / 1000 + " s (" + Math.round((double) HashJoiner.searchForJoin / (double) duration * 100) + "%)");
        logger.info("    deriveJoinBefAft Time: " + (double) this.worker.getTree().getMerger().deriveJoinBefAft / 1000 + " s (" + Math.round((double) this.worker.getTree().getMerger().deriveJoinBefAft / (double) duration * 100) + "%)");
        logger.info("Total update data time: " + (double) updateTime / 1000 + " s (" + Math.round((double) updateTime / (double) duration * 100) + "%)");
        logger.info("        concateTime Time: " + (double) Table.concateTime / 1000 + " s (" + Math.round((double) Table.concateTime / (double) duration * 100) + "%)");
        logger.info("            concateRebuilTime Time: " + (double) Table.concateRebuilTime / 1000 + " s (" + Math.round((double) Table.concateRebuilTime / (double) duration * 100) + "%)");
        logger.info("            addRowMergeTime Time: " + (double) Table.addRowMergeTime / 1000 + " s (" + Math.round((double) Table.addRowMergeTime / (double) duration * 100) + "%)");
        logger.info("                clearRowsTime Time: " + (double) Table.clearRowsTime / 1000 + " s (" + Math.round((double) Table.clearRowsTime / (double) duration * 100) + "%)");
        logger.info("                removeRowsAndIndexTime Time: " + (double) Table.removeRowsAndIndexTime / 1000 + " s (" + Math.round((double) Table.removeRowsAndIndexTime / (double) duration * 100) + "%)");
        logger.info("                addRowsTime Time: " + (double) Table.addRowsTime / 1000 + " s (" + Math.round((double) Table.addRowsTime / (double) duration * 100) + "%)");
        logger.info("                merge_simple Time: " + (double) MapMerger.merge_simple / 1000 + " s (" + Math.round((double) MapMerger.merge_simple / (double) duration * 100) + "%)");
    }

    /**
     * Prints the current count of the results processed by the worker.
     */
    public void printResultCNT() {
        logger.info("RESULT: " + worker.getResultCNT());
    }

    /**
     * Get the current count of the results processed by the worker.
     */
    public long getResultCNT() {
        return worker.getResultCNT();
    }

    /**
     * Prints the average processing time for the events handled by the worker.
     */
    public void printAVGprocessLatency() {
        logger.info("AVG-latencyTime: " + worker.getTree().getRoot().getAVGprocessLatency() + " ns");
    }

    /**
     * Get the average processing time for the events handled by the worker.
     */
    public long getAVGprocessLatency() {
        return worker.getTree().getRoot().getAVGprocessLatency();
    }
}
